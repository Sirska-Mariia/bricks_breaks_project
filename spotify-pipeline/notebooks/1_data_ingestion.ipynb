{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c58bc92-753a-44c7-a6de-d84d126f4dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Data ingestion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47ee76d8-66f0-44fe-8858-c5bf38a3577c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dataset: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe9bbde-615c-4e8d-96d4-8e1f533e6510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Architecture Decision: Batch vs. Streaming\n",
    "\n",
    "Ingestion Strategy: Batch ingestion (spark.read)\n",
    "\n",
    "Justification: The source data is a static, CSV dataset (dataset.csv) provided via Kaggle. Since the data is not arriving continuously in real-time and requires a one-time load for analysis, Batch ingestion is the most efficient and appropriate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58dcc3b9-3fce-4c89-a13a-933bdc27ccee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "source_path = \"/Workspace/Shared/bricks_breaks_project/spotify-pipeline/notebooks/data/dataset.csv\" \n",
    "bronze_table_path = \"default.spotify_bronze\"\n",
    "quarantine_table_path = \"default.spotify_quarantine\"\n",
    "\n",
    "# Define Schema\n",
    "spotify_schema = StructType([\n",
    "    StructField(\"row_id\", IntegerType(), True), \n",
    "    StructField(\"track_id\", StringType(), False),\n",
    "    StructField(\"artists\", StringType(), True),\n",
    "    StructField(\"album_name\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True),\n",
    "    StructField(\"popularity\", IntegerType(), True),\n",
    "    StructField(\"duration_ms\", IntegerType(), True),\n",
    "    StructField(\"explicit\", BooleanType(), True),\n",
    "    StructField(\"danceability\", FloatType(), True),\n",
    "    StructField(\"energy\", FloatType(), True),\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"mode\", IntegerType(), True),\n",
    "    StructField(\"speechiness\", FloatType(), True),\n",
    "    StructField(\"acousticness\", FloatType(), True),\n",
    "    StructField(\"instrumentalness\", FloatType(), True),\n",
    "    StructField(\"liveness\", FloatType(), True),\n",
    "    StructField(\"valence\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", IntegerType(), True),\n",
    "    StructField(\"track_genre\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Ingestion (Batch Mode)\n",
    "raw_df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .schema(spotify_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"mode\", \"PERMISSIVE\") \n",
    "    .load(source_path)\n",
    ")\n",
    "\n",
    "df_with_meta = raw_df \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    "\n",
    "is_valid_rule = (col(\"track_id\").isNotNull()) & (col(\"duration_ms\") > 0)\n",
    "\n",
    "valid_df = df_with_meta.filter(is_valid_rule)\n",
    "invalid_df = df_with_meta.filter(~is_valid_rule) \n",
    "\n",
    "# Storage (Bronze & Quarantine)\n",
    "\n",
    "# Saving Valid data to bronze\n",
    "valid_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(bronze_table_path)\n",
    "\n",
    "# saving invalid data to quarantine\n",
    "if invalid_df.count() > 0:\n",
    "    invalid_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(quarantine_table_path)\n",
    "    print(f\"WARNING: {invalid_df.count()} invalid records quarantined to {quarantine_table_path}\")\n",
    "else:\n",
    "    print(\"Success: No invalid records found.\")\n",
    "print(f\"Success: {valid_df.count()} valid records ingested to {bronze_table_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5463422945280324,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
